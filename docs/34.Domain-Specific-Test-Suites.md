# Test Suites by Domain

> **Relevant source files**
> * [examples/data/endings.json](../examples/data/endings.json)
> * [examples/data/novels.json](../examples/data/novels.json)
> * [examples/data/television series.json](../examples/data/television series.json)
> * [tests/utils/dump_runner.py](../tests/utils/dump_runner.py)

This page documents the domain-specific test suites that validate translation accuracy for each resolver in the ArWikiCats system. Tests are organized by translation domain (nationalities, ministers, films, countries, etc.) and use data-driven testing with pytest parametrization. For information about test data organization and JSON file structure, see [Test Data Organization](33.Test-Organization.md). For test execution utilities and markers, see [Test Utilities and Markers](35.Test-Utilities.md)

## Test Suite Organization

The testing infrastructure mirrors the resolver chain architecture, with dedicated test files for each major domain. Test suites range from ~57 test cases (ministers) to 800+ cases (nationalities), reflecting the complexity and coverage needs of each resolver.

```mermaid
flowchart TD

NATS["tests/.../test_nats_v2.py<br>Nationality Tests<br>800+ cases"]
YEARS["tests/.../test_labs_years.py<br>Year Pattern Tests<br>Decade/Century validation"]
MINISTERS["tests/event_lists/test_ministers.py<br>Ministers Tests<br>57 patterns"]
FILMS["tests/.../test_tyty.py<br>Films Tests<br>Genre combinations"]
COUNTRIES["tests/.../test_countries_names2.py<br>Country Tests<br>24,479 entries"]
RELATIONS["tests/.../test_relations.py<br>Relations Tests<br>Bilateral patterns"]
ARLAB["tests/.../test_ar_lab_big_data.py<br>ar_lab Tests<br>1000+ integration"]
PAPUA["tests/event_lists/papua_new_guinea/<br>test_papua_new_guinea.py<br>Country-specific"]
JSON_5K["examples/data/5k.json<br>5000+ pairs<br>Main integration"]
JSON_2025["examples/data/2025-11-28.json<br>Supplementary data"]
JSON_1K["examples/data/1k.json<br>Elections focus"]
JSON_NOVELS["examples/data/novels.json<br>Novel categories"]
JSON_FILMS["examples/data/films_with_time.json<br>Temporal film data"]
JSON_RELATIONS["examples/data/relations_data.json<br>Bilateral relations"]
JSON_TEAMS["examples/data/teams_to_test.json<br>Sports teams"]
RESOLVE["resolve_label_ar<br>Main test function"]
DUMP["one_dump_test<br>Bulk validation"]
DIFF["dump_diff<br>Regression detection"]

JSON_5K --> NATS
JSON_5K --> YEARS
JSON_5K --> COUNTRIES
JSON_5K --> ARLAB
JSON_2025 --> ARLAB
JSON_1K --> ARLAB
JSON_NOVELS --> FILMS
JSON_FILMS --> FILMS
JSON_RELATIONS --> RELATIONS
JSON_TEAMS --> ARLAB
NATS --> RESOLVE
YEARS --> RESOLVE
MINISTERS --> RESOLVE
FILMS --> RESOLVE
COUNTRIES --> RESOLVE
RELATIONS --> RESOLVE
ARLAB --> RESOLVE
PAPUA --> RESOLVE

subgraph subGraph2 ["Test Interface"]
    RESOLVE
    DUMP
    DIFF
    RESOLVE --> DUMP
    DUMP --> DIFF
end

subgraph subGraph1 ["Test Data Sources"]
    JSON_5K
    JSON_2025
    JSON_1K
    JSON_NOVELS
    JSON_FILMS
    JSON_RELATIONS
    JSON_TEAMS
end

subgraph subGraph0 ["Test Suites by Domain"]
    NATS
    YEARS
    MINISTERS
    FILMS
    COUNTRIES
    RELATIONS
    ARLAB
    PAPUA
end
```

**Sources:** [tests/event_lists/test_2.py](../tests/event_lists/test_2.py)

 [tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py](../tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py)

 [tests/event_lists/test_ministers.py](../tests/event_lists/test_ministers.py)

 [examples/data/5k.json](../examples/data/5k.json)

 [examples/data/2025-11-28.json](../examples/data/2025-11-28.json)

 [examples/data/1k.json](../examples/data/1k.json)

## Nationality Tests

The nationality resolver tests validate 799 nationality variants across multiple pattern types, making this the most comprehensive domain-specific test suite.

### Test Files and Coverage

| Test File | Location | Coverage | Cases |
| --- | --- | --- | --- |
| `test_nats_v2.py` | `tests/new_resolvers/nationalities_resolvers/nationalities_v2/` | Core nationality patterns | 800+ |
| `test_nats_v2_jobs.py` | Same directory | Nationality + occupation | 200+ |
| `test_2.py` | `tests/event_lists/` | Yemeni-specific patterns | 600+ |

### Test Data Dictionary Structure

The nationality tests use multiple data dictionaries organized by grammatical form:

```mermaid
flowchart TD

TEST_MALES["test_data_males<br>{'{en} non profit publishers':<br>'ناشرون غير ربحيون {males}'}"]
TEST_AR["test_data_ar<br>{'{en} music groups':<br>'فرق موسيقى {female}'}"]
TEST_FEMALE["test_data_female_music<br>{'{en} musical duos':<br>'فرق موسيقية ثنائية {female}'}"]
TEST_MALE["test_data_male<br>{'{en} cuisine':<br>'مطبخ {male}'}"]
TEST_THE_MALE["test_data_the_male<br>{'{en} occupation':<br>'الاحتلال {the_male}'}"]
PARAMETRIZE["@pytest.mark.parametrize<br>'category, expected'"]
RESOLVE_BY_NATS["resolve_by_nats<br>function"]
ASSERT["assert result == expected"]

TEST_MALES --> PARAMETRIZE
TEST_AR --> PARAMETRIZE
TEST_FEMALE --> PARAMETRIZE
TEST_MALE --> PARAMETRIZE
TEST_THE_MALE --> PARAMETRIZE

subgraph subGraph1 ["Test Execution"]
    PARAMETRIZE
    RESOLVE_BY_NATS
    ASSERT
    PARAMETRIZE --> RESOLVE_BY_NATS
    RESOLVE_BY_NATS --> ASSERT
end

subgraph subGraph0 ["Nationality Test Data Structure"]
    TEST_MALES
    TEST_AR
    TEST_FEMALE
    TEST_MALE
    TEST_THE_MALE
end
```

**Key Test Patterns:**

```css
# From test_nats_v2.py:8-24
test_data_males = {
    "yemeni non profit publishers": "ناشرون غير ربحيون يمنيون",
    "yemeni government officials": "مسؤولون حكوميون يمنيون",
}

test_data_ar = {
    "yemeni music groups": "فرق موسيقى يمنية",
    "yemeni rock musical groups": "فرق موسيقى روك يمنية",
}

test_data_the_male = {
    "yemeni occupation": "الاحتلال اليمني",
    "yemeni premier league": "الدوري اليمني الممتاز",
}
```

### Yemeni-Specific Test Suite

The file `test_2.py` contains an exhaustive test suite for Yemeni nationality patterns, covering 600+ category types:

```css
# From test_2.py:7-211
fast_data = {
    "yemeni sports": "ألعاب رياضية يمنية",
    "yemeni buildings": "مباني يمنية",
    "yemeni elections": "انتخابات يمنية",
    "yemeni musical groups": "فرق موسيقية يمنية",
    # ... 600+ more patterns
}
```

This data validates nationality-specific patterns across:

* Cultural categories (music, media, organizations)
* Geographic categories (islands, mountains, lakes)
* Event categories (competitions, festivals, elections)
* Occupation categories (businesspeople, journalists)

**Sources:** [tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py L1-L300](../tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py#L1-L300)

 [tests/event_lists/test_2.py L1-L601](../tests/event_lists/test_2.py#L1-L601)

 [tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2_jobs.py](../tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2_jobs.py)

## Ministers and Politics Tests

The ministers test suite validates translation of political office titles, focusing on grammatical correctness with the Arabic definite article (ال).

### Test Structure

```css
# From test_ministers.py:9-16
examples_1 = {
    "Ministers for foreign affairs of Papua New Guinea": "وزراء شؤون خارجية بابوا غينيا الجديدة",
    "Justice ministers of Papua New Guinea": "وزراء عدل بابوا غينيا الجديدة",
    "Agriculture ministers of Antigua and Barbuda": "وزراء زراعة أنتيغوا وباربودا",
    "Energy ministers of Antigua and Barbuda": "وزراء طاقة أنتيغوا وباربودا",
}
```

### Ministers Key Structure

The test data references `ministers_keys` dictionary which contains both definite and indefinite forms:

```mermaid
flowchart TD

KEY["ministers_keys<br>Dictionary"]
AGRICULTURE["'agriculture':<br>{no_al: 'زراعة',<br>with_al: 'الزراعة'}"]
DEFENSE["'defence':<br>{no_al: 'دفاع',<br>with_al: 'الدفاع'}"]
FOREIGN["'foreign affairs':<br>{no_al: 'شؤون خارجية',<br>with_al: 'الشؤون الخارجية'}"]
JUSTICE["'justice':<br>{no_al: 'عدل',<br>with_al: 'العدل'}"]
INPUT["'Justice ministers of<br>Papua New Guinea'"]
RESOLVE["ministers resolver"]
OUTPUT["'وزراء عدل<br>بابوا غينيا الجديدة'"]

JUSTICE --> RESOLVE

subgraph subGraph1 ["Test Validation"]
    INPUT
    RESOLVE
    OUTPUT
    INPUT --> RESOLVE
    RESOLVE --> OUTPUT
end

subgraph subGraph0 ["ministers_keys Structure"]
    KEY
    AGRICULTURE
    DEFENSE
    FOREIGN
    JUSTICE
    KEY --> AGRICULTURE
    KEY --> DEFENSE
    KEY --> FOREIGN
    KEY --> JUSTICE
end
```

The test suite validates ~94 ministry types with proper article usage:

| Ministry Type | No Article (no_al) | With Article (with_al) |
| --- | --- | --- |
| agriculture | زراعة | الزراعة |
| defence | دفاع | الدفاع |
| foreign affairs | شؤون خارجية | الشؤون الخارجية |
| justice | عدل | العدل |
| interior | داخلية | الداخلية |

**Sources:** [tests/event_lists/test_ministers.py L1-L50](../tests/event_lists/test_ministers.py#L1-L50)

 [ArWikiCats/translations/politics/ministers.py L1-L104](../ArWikiCats/translations/politics/ministers.py#L1-L104)

## Year Pattern Tests

Year pattern tests validate temporal expression translation including decades, centuries, and specific years. These tests ensure correct Arabic numeral and time period formatting.

### Test Coverage

```mermaid
flowchart TD

DECADES["Decade Tests<br>'1970s albums'<br>→ 'ألبومات عقد 1970'"]
CENTURIES["Century Tests<br>'11th-century composers'<br>→ 'ملحنون في القرن 11'"]
YEARS["Specific Years<br>'2010 births'<br>→ 'مواليد 2010'"]
RANGES["Year Ranges<br>'2010-20 in British football'<br>→ 'كرة القدم البريطانية في 2010-20'"]
JSON_5K_YEARS["5k.json<br>Temporal patterns"]
JSON_2025_YEARS["2025-11-28.json<br>Century patterns"]
LABS_YEARS["LabsYears class"]
CONVERT_TIME["convert_time_to_arabic<br>function"]
YEAR_DATA["YEAR_DATA<br>dictionary"]

JSON_5K_YEARS --> DECADES
JSON_5K_YEARS --> YEARS
JSON_2025_YEARS --> CENTURIES
DECADES --> LABS_YEARS
CENTURIES --> LABS_YEARS
YEARS --> LABS_YEARS
RANGES --> LABS_YEARS

subgraph Validation ["Validation"]
    LABS_YEARS
    CONVERT_TIME
    YEAR_DATA
    LABS_YEARS --> CONVERT_TIME
    LABS_YEARS --> YEAR_DATA
end

subgraph subGraph1 ["Test Data Sources"]
    JSON_5K_YEARS
    JSON_2025_YEARS
end

subgraph subGraph0 ["Year Pattern Test Types"]
    DECADES
    CENTURIES
    YEARS
    RANGES
end
```

### Example Test Cases

From the 5k.json dataset:

```markdown
# Decade patterns
"Category:2010s Massachusetts elections": "تصنيف:انتخابات ماساتشوستس في عقد 2010"
"Category:1970s in Australian tennis": "تصنيف:كرة المضرب الأسترالية في عقد 1970"

# Century patterns
"Category:11th-century composers": "تصنيف:ملحنون في القرن 11"
"Category:13th-century Italian judges": "تصنيف:قضاة إيطاليون في القرن 13"

# Year ranges
"Category:2010-20 in British football": "تصنيف:كرة القدم البريطانية في 2010-20"
```

**Sources:** [examples/data/5k.json L1-L50](../examples/data/5k.json#L1-L50)

 [examples/data/2025-11-28.json L1-L40](../examples/data/2025-11-28.json#L1-L40)

## Films and Television Tests

Films and television tests validate genre combinations, temporal patterns, and nationality-based film categories.

### Test Data Files

| File | Purpose | Example Patterns |
| --- | --- | --- |
| `films_with_time.json` | Films with temporal data | `2010s fantasy films`, `2010 American films` |
| `television series.json` | TV series patterns | `Nigerian television series`, `2010s Swedish television series` |
| `endings.json` | TV series endings | `2010 Brazilian television series endings` |
| `novels.json` | Novel categories (film adaptations) | `2010 French novels`, `Films based on novels by Thomas Pynchon` |

### Film Test Pattern Structure

```css
# From films_with_time.json:1-7
{
    "Category:2010s fantasy novels": "تصنيف:روايات فانتازيا في عقد 2010",
    "Category:2010s science fiction novels": "تصنيف:روايات خيال علمي في عقد 2010",
    "Category:2010 fantasy novels": "تصنيف:روايات فانتازيا في 2010",
    "Category:2010s mystery films": "تصنيف:أفلام غموض في عقد 2010",
    "Category:2010s pornographic films": "تصنيف:أفلام إباحية في عقد 2010"
}
```

### Films_key_CAO Coverage

The test data validates against the `Films_key_CAO` dictionary containing 13,146 film genre and type translations:

```mermaid
flowchart TD

GENRE["Genre Films<br>fantasy, science fiction,<br>mystery, action"]
NAT_GENRE["Nationality + Genre<br>'American action films'"]
TIME_GENRE["Temporal + Genre<br>'2010s fantasy films'"]
NAT_TIME_GENRE["All Three<br>'2010s American action films'"]
FILMS_KEY_CAO["Films_key_CAO<br>13,146 entries"]
RESOLVE_FILMS["resolve_films_labels<br>function"]
MULTI_FORMATTER["MultiDataFormatterBaseYear<br>Nationality + Year"]

GENRE --> RESOLVE_FILMS
NAT_GENRE --> RESOLVE_FILMS
TIME_GENRE --> RESOLVE_FILMS
NAT_TIME_GENRE --> RESOLVE_FILMS

subgraph subGraph1 ["Test Validation"]
    FILMS_KEY_CAO
    RESOLVE_FILMS
    MULTI_FORMATTER
    FILMS_KEY_CAO --> RESOLVE_FILMS
    RESOLVE_FILMS --> MULTI_FORMATTER
end

subgraph subGraph0 ["Film Test Categories"]
    GENRE
    NAT_GENRE
    TIME_GENRE
    NAT_TIME_GENRE
end
```

**Sources:** [examples/data/films_with_time.json](../examples/data/films_with_time.json)

 [examples/data/television series.json](../examples/data/television series.json)

 [examples/data/endings.json](../examples/data/endings.json)

 [examples/data/novels.json L1-L35](../examples/data/novels.json#L1-L35)

## Country-Specific Tests

Country-specific tests provide comprehensive validation for individual countries' categories, ensuring all nationality patterns work correctly.

### Papua New Guinea Test Suite

The Papua New Guinea test suite demonstrates the structure of country-specific testing:

```css
# From test_papua_new_guinea.py:7-22
data_skip = {
    "Category:Defunct airports in Papua New Guinea": "تصنيف:مطارات سابقة في بابوا غينيا الجديدة",
    "Category:April 2023 in Papua New Guinea": "تصنيف:بابوا غينيا الجديدة في أبريل 2023",
}

data_0 = {
    "Category:Papua New Guinea men's international soccer players": "تصنيف:لاعبو كرة قدم دوليون من بابوا غينيا الجديدة",
    "Category:Papua New Guinea women's international soccer players": "تصنيف:لاعبات كرة قدم دوليات من بابوا غينيا الجديدة",
}
```

### Country Test Coverage Pattern

```mermaid
flowchart TD

PEOPLE["People Categories<br>'Papua New Guinea men'"]
GEO["Geography Categories<br>'Rivers of Papua New Guinea'"]
SPORTS["Sports Categories<br>'rugby league in PNG'"]
POLITICS["Political Categories<br>'Ministers of PNG'"]
CULTURE["Cultural Categories<br>'Museums in PNG'"]
DATA_SKIP["data_skip<br>Known issues"]
DATA_0["data_0<br>Main test cases"]
DATA_1["data_1<br>Additional patterns"]
PARAMETRIZE["@pytest.mark.parametrize"]
SKIP_MARKER["@pytest.mark.skip2"]

PEOPLE --> DATA_0
GEO --> DATA_0
SPORTS --> DATA_0
POLITICS --> DATA_0
CULTURE --> DATA_0
DATA_0 --> PARAMETRIZE
DATA_1 --> PARAMETRIZE
DATA_SKIP --> SKIP_MARKER

subgraph subGraph1 ["Test Data Organization"]
    DATA_SKIP
    DATA_0
    DATA_1
end

subgraph subGraph0 ["Country-Specific Test Structure"]
    PEOPLE
    GEO
    SPORTS
    POLITICS
    CULTURE
end
```

The test file validates country-specific patterns including:

* **Geographic entities**: Rivers, regions, atolls, calderas
* **Political structures**: Ministers, governors, parliament constituencies
* **Sports teams**: Rugby league teams, national teams
* **Organizations**: Catholic schools, military units, missions
* **Infrastructure**: Airports, cricket grounds, military airfields

**Sources:** [tests/event_lists/papua_new_guinea/test_papua_new_guinea.py L1-L200](../tests/event_lists/papua_new_guinea/test_papua_new_guinea.py#L1-L200)

## Relations Tests

Relations tests validate bilateral relationship categories between countries.

### Relations Data Structure

```css
# From relations_data.json:1-5
{
    "north macedonia–qatar relations": "تصنيف:العلاقات القطرية المقدونية الشمالية",
    "north macedonia–serbia border crossings": "تصنيف:معابر الحدود الصربية المقدونية الشمالية",
    "north macedonia–serbia border": "تصنيف:الحدود الصربية المقدونية الشمالية",
    "north macedonia–serbia relations": "تصنيف:العلاقات الصربية المقدونية الشمالية"
}
```

### Relations Pattern Types

```mermaid
flowchart TD

REL["Basic Relations<br>'Country A-Country B relations'"]
BORDER["Border Patterns<br>'Country A-Country B border'"]
CROSSINGS["Border Crossings<br>'Country A-Country B border crossings'"]
TREATIES["Treaties<br>'Country A-Country B treaties'"]
FORMAT_DOUBLE["FormatDataDoubleV2<br>Dual placeholders"]
PUT_LABEL_LAST["put_label_last<br>Reorder labels"]
COUNTRY_DATA["formatted_data_en_ar<br>Country translations"]

REL --> FORMAT_DOUBLE
BORDER --> FORMAT_DOUBLE
CROSSINGS --> FORMAT_DOUBLE
TREATIES --> FORMAT_DOUBLE

subgraph subGraph1 ["Validation Logic"]
    FORMAT_DOUBLE
    PUT_LABEL_LAST
    COUNTRY_DATA
    FORMAT_DOUBLE --> PUT_LABEL_LAST
    FORMAT_DOUBLE --> COUNTRY_DATA
end

subgraph subGraph0 ["Relations Test Patterns"]
    REL
    BORDER
    CROSSINGS
    TREATIES
end
```

**Sources:** [examples/data/relations_data.json L1-L10](../examples/data/relations_data.json#L1-L10)

## Integration Tests (ar_lab)

The `ar_lab` test suite provides end-to-end integration testing across all resolvers with 1000+ complex test cases.

### Test Data Sources for Integration

```mermaid
flowchart TD

JSON_5K_INT["5k.json<br>5000+ pairs<br>Primary integration"]
JSON_2025_INT["2025-11-28.json<br>Supplementary"]
JSON_1K_INT["1k.json<br>Elections focus"]
JSON_TEAMS["teams_to_test.json<br>Sports teams"]
AR_LAB_TEST["test_ar_lab_big_data.py<br>1000+ cases"]
RESOLVE_LABEL["resolve_label_ar<br>Full pipeline"]
ALL_RESOLVERS["All 7 resolvers<br>Year, Nat, Country,<br>Job, Sport, Film, Minister"]
SEPARATOR_FIX["separator_lists_fixing<br>Preposition insertion"]
ADD_IN_TAB["add_in_tab<br>Grammar corrections"]
FIXLABEL["fixlabel<br>Article agreement"]

JSON_5K_INT --> AR_LAB_TEST
JSON_2025_INT --> AR_LAB_TEST
JSON_1K_INT --> AR_LAB_TEST
JSON_TEAMS --> AR_LAB_TEST
ALL_RESOLVERS --> SEPARATOR_FIX

subgraph Validation ["Validation"]
    SEPARATOR_FIX
    ADD_IN_TAB
    FIXLABEL
    SEPARATOR_FIX --> ADD_IN_TAB
    ADD_IN_TAB --> FIXLABEL
end

subgraph subGraph1 ["Integration Test Flow"]
    AR_LAB_TEST
    RESOLVE_LABEL
    ALL_RESOLVERS
    AR_LAB_TEST --> RESOLVE_LABEL
    RESOLVE_LABEL --> ALL_RESOLVERS
end

subgraph subGraph0 ["Integration Test Data"]
    JSON_5K_INT
    JSON_2025_INT
    JSON_1K_INT
    JSON_TEAMS
end
```

### Integration Test Coverage

The integration tests validate:

1. **Multi-domain categories**: Categories requiring multiple resolvers (e.g., "2010s American football coaches")
2. **Complex grammar**: Categories with prepositions, separators, and Arabic article agreement
3. **Edge cases**: Unusual category structures, multiple separators, special characters
4. **End-to-end accuracy**: Complete translation pipeline from raw input to final Arabic output

**Example Complex Cases:**

```markdown
# From 5k.json - Categories requiring multiple resolvers
"Category:2010s American sports-people": "تصنيف:رياضيون أمريكيون في عقد 2010"  # Year + Nationality
"Category:Ministers for foreign affairs of Papua New Guinea": "تصنيف:وزراء شؤون خارجية بابوا غينيا الجديدة"  # Ministers + Country
"Category:2010s fantasy novels": "تصنيف:روايات فانتازيا في عقد 2010"  # Year + Film genre
```

**Sources:** [examples/data/5k.json L1-L100](../examples/data/5k.json#L1-L100)

 [examples/data/2025-11-28.json L1-L50](../examples/data/2025-11-28.json#L1-L50)

 [examples/data/1k.json L1-L50](../examples/data/1k.json#L1-L50)

 [examples/data/teams_to_test.json](../examples/data/teams_to_test.json)

## Test Execution Patterns

All domain-specific tests follow a consistent execution pattern using pytest parametrization and helper utilities.

### Common Test Structure

```python
# Common pattern across all test files
import pytest
from load_one_data import dump_diff, one_dump_test
from ArWikiCats import resolve_label_ar

test_data = {
    "input category": "expected output",
    # ... more test cases
}

@pytest.mark.parametrize("category, expected", test_data.items())
def test_domain(category, expected):
    result = resolve_label_ar(category)
    assert result == expected
```

### Test Helper Functions

```mermaid
flowchart TD

ONE_DUMP["one_dump_test<br>Bulk validation<br>Run all test cases"]
DUMP_DIFF["dump_diff<br>Regression detection<br>Compare results"]
RESOLVE["resolve_label_ar<br>Main test interface<br>Full pipeline"]
TEST_FILE["test_*.py"]
PARAMETRIZE["@pytest.mark.parametrize"]
ASSERT["assert result == expected"]
SUCCESS["Test Pass"]
REGRESSION["Regression Report<br>Expected vs Actual"]
COVERAGE["Coverage Metrics"]

PARAMETRIZE --> RESOLVE
RESOLVE --> ASSERT
ASSERT --> SUCCESS
ASSERT --> REGRESSION
TEST_FILE --> ONE_DUMP
DUMP_DIFF --> REGRESSION
DUMP_DIFF --> COVERAGE

subgraph subGraph2 ["Test Output"]
    SUCCESS
    REGRESSION
    COVERAGE
end

subgraph subGraph1 ["Test Execution"]
    TEST_FILE
    PARAMETRIZE
    ASSERT
    TEST_FILE --> PARAMETRIZE
end

subgraph subGraph0 ["Test Helper Functions"]
    ONE_DUMP
    DUMP_DIFF
    RESOLVE
    ONE_DUMP --> DUMP_DIFF
end
```

### Test Markers Usage

Tests use pytest markers for selective execution:

| Marker | Purpose | Usage |
| --- | --- | --- |
| `@pytest.mark.fast` | Quick unit tests | Development testing |
| `@pytest.mark.slow` | Comprehensive tests | CI/CD validation |
| `@pytest.mark.dump` | Full dataset validation | Regression testing |
| `@pytest.mark.skip2` | Known issues | Temporary exclusion |
| `@pytest.mark.parametrize` | Data-driven tests | All domain tests |

**Sources:** [tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py L1-L10](../tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py#L1-L10)

 [tests/event_lists/test_2.py L1-L10](../tests/event_lists/test_2.py#L1-L10)

 [tests/event_lists/test_ministers.py L1-L10](../tests/event_lists/test_ministers.py#L1-L10)

## Domain Coverage Summary

The following table summarizes test coverage across all domains:

| Domain | Test File(s) | Test Cases | Data Sources | Importance |
| --- | --- | --- | --- | --- |
| Nationalities | `test_nats_v2.py`, `test_2.py`, `test_nats_v2_jobs.py` | 800+ | 799 nationality variants | 129.38 |
| Ministers/Politics | `test_ministers.py` | 57 | `ministers_keys` (94 entries) | 88.16 |
| Films/TV | `test_tyty.py`, films JSON files | 500+ | `Films_key_CAO` (13,146 entries) | 85.39 |
| Year Patterns | `test_labs_years.py` | 300+ | `YEAR_DATA` dictionary | 81.33 |
| Countries | `test_countries_names2.py`, country-specific tests | 1000+ | `NEW_P17_FINAL` (24,479 entries) | 72.63 |
| Relations | `test_relations.py` | 200+ | `relations_data.json` | 65.89 |
| Integration | `test_ar_lab_big_data.py` | 5000+ | Multiple JSON files | 86.76 |

This comprehensive test coverage ensures translation accuracy across ~100,000+ category patterns, with domain-specific validation guaranteeing correct grammatical forms, preposition usage, and Arabic article agreement.

**Sources:** [tests/event_lists/test_2.py](../tests/event_lists/test_2.py)

 [tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py](../tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py)

 [tests/event_lists/test_ministers.py](../tests/event_lists/test_ministers.py)

 [examples/data/5k.json](../examples/data/5k.json)

 [ArWikiCats/translations/politics/ministers.py](../ArWikiCats/translations/politics/ministers.py)
