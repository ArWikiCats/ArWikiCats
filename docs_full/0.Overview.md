
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [.github/copilot-instructions.md](.github/copilot-instructions.md)
- [.github/workflows/python-publish.yml](.github/workflows/python-publish.yml)
- [ArWikiCats/config.py](../ArWikiCats/config.py)
- [ArWikiCats/jsons/population/pop_All_2018.json](../ArWikiCats/jsons/population/pop_All_2018.json)
- [ArWikiCats/main_processers/main_resolve.py](../ArWikiCats/main_processers/main_resolve.py)
- [CLAUDE.md](CLAUDE.md)
- [README.md](README.md)
- [changelog.md](changelog.md)
- [tests_require_fixes/test_papua_new_guinean.py](tests_require_fixes/test_papua_new_guinean.py)
- [tests_require_fixes/test_skip_data_all.py](tests_require_fixes/test_skip_data_all.py)
- [tests_require_fixes/text_to_fix.py](tests_require_fixes/text_to_fix.py)

</details>



## Purpose and Scope

ArWikiCats is a Python library that automatically translates English Wikipedia category names into standardized Arabic equivalents. The system processes category labels through a multi-stage resolution pipeline, leveraging extensive translation dictionaries covering temporal patterns, geographic entities, occupations, sports, nationalities, and media-related terms.

This page provides a high-level understanding of the system's architecture, capabilities, and core components. For detailed information about specific subsystems:
- Installation and basic usage: see [Getting Started](1.Getting-Started.md)
- Detailed architecture: see [Architecture](2.Architecture.md)
- Translation data organization: see [Translation Data](6.Translation-Data.md)
- Resolver implementations: see [Resolver System](14.Resolver-System.md)
- Template formatting engine: see [Formatting System](22.Formatting-System.md)

**Sources:** [README.md:1-560](), [CLAUDE.md:1-227](), [.github/copilot-instructions.md:1-121]()

---

## System Capabilities

The ArWikiCats translation engine maintains comprehensive translation datasets across multiple domains:

| Domain | Dataset Size | Key Examples |
|--------|-------------|--------------|
| **Jobs and Occupations** | 96,552 male entries, extensive female mappings | footballers, painters, scientists, religious occupations |
| **Sports** | 431 sport key records, 571 job variants | football, basketball, teams, players, competitions |
| **Nationalities** | 843 entries, 18 lookup tables | British, American, Egyptian (male/female/plural/definite forms) |
| **Geographic Data** | 68,981 entries | cities, regions, countries, counties, US states |
| **Films and Television** | 13,146 entries | film genres, TV series, directors, actors |
| **Test Coverage** | 28,500+ tests | unit, integration, end-to-end tests achieving 91% coverage |

The system handles complex multi-element categories such as:
- Temporal + Nationality + Occupation: "2010 British football players"
- Country + Year + Event: "1990s establishments in France"
- Nationality + Sport + Team: "Argentine football club managers"

**Sources:** [README.md:42-113](), [changelog.md:1-80]()

---

## High-Level Architecture

The system follows a layered architecture with specialized resolver chains that process categories in priority order:

```mermaid
graph TB
    subgraph "Public API Layer"
        API1["resolve_arabic_category_label()"]
        API2["batch_resolve_labels()"]
        API3["resolve_label_ar()"]
        API4["EventProcessor"]
    end

    subgraph "Main Resolution Coordinator"
        MainResolve["main_processers/main_resolve.py<br/>resolve_label()"]
        Cache["@lru_cache(maxsize=50000)"]
    end

    subgraph "Input Processing"
        ChangeCAT["format_bots/change_cat()"]
        FilterEN["fix/filter_en.py<br/>is_category_allowed()"]
    end

    subgraph "Resolution Pipeline (Priority Order)"
        PatternResolvers["patterns_resolvers/<br/>all_patterns_resolvers()"]
        NewResolvers["new_resolvers/<br/>all_new_resolvers()"]
        UnivResolver["sub_new_resolvers/<br/>university_resolver"]
        LegacyResolver["legacy_bots/<br/>legacy_resolvers()"]
    end

    subgraph "Output Processing"
        FixLabel["fix/fixtitle.py<br/>fixlabel()"]
        Cleanse["fix/<br/>cleanse_category_label()"]
    end

    subgraph "Translation Data Sources"
        TranslationsGeo["translations/geo/<br/>CITY_TRANSLATIONS"]
        TranslationsJobs["translations/jobs/<br/>jobs_mens_data"]
        TranslationsSports["translations/sports/<br/>SPORT_KEY_RECORDS"]
        TranslationsNats["translations/nats/<br/>All_Nat"]
    end

    API1 --> MainResolve
    API2 --> MainResolve
    API3 --> MainResolve
    API4 --> MainResolve

    MainResolve --> Cache
    Cache --> ChangeCAT
    ChangeCAT --> FilterEN

    FilterEN --> PatternResolvers
    PatternResolvers -->|"no match"| NewResolvers
    NewResolvers -->|"no match"| UnivResolver
    UnivResolver -->|"no match"| LegacyResolver

    PatternResolvers --> FixLabel
    NewResolvers --> FixLabel
    UnivResolver --> FixLabel
    LegacyResolver --> FixLabel

    FixLabel --> Cleanse

    NewResolvers --> TranslationsGeo
    NewResolvers --> TranslationsJobs
    NewResolvers --> TranslationsSports
    NewResolvers --> TranslationsNats
```

**Architecture Overview**

The system is organized into five primary layers:

1. **Public API Layer** - User-facing functions in [ArWikiCats/__init__.py]() that provide simple interfaces for single-category and batch translation
2. **Main Resolution Coordinator** - [main_processers/main_resolve.py:33-93]() orchestrates the resolution pipeline with LRU caching
3. **Input Processing** - Normalizes categories and filters invalid inputs before resolution
4. **Resolution Pipeline** - Prioritized chain of specialized resolvers that attempt pattern matching
5. **Output Processing** - Applies Arabic formatting rules and cleansing to finalized labels

**Sources:** [ArWikiCats/main_processers/main_resolve.py:1-106](), [CLAUDE.md:69-103]()

---

## Resolution Pipeline Flow

The category resolution process follows a strict priority order to prevent conflicts and ensure accurate translations:

```mermaid
flowchart TD
    Input["Input: English Category<br/>'Category:2010 British footballers'"]

    subgraph Normalization["Normalization Stage"]
        ChangeCat["change_cat()<br/>Remove 'Category:' prefix<br/>Lowercase, clean spaces"]
        Filter["filter_en.is_category_allowed()<br/>Check if category is valid"]
    end

    subgraph PatternStage["Pattern Resolution (Priority 1)"]
        AllPatterns["all_patterns_resolvers()<br/>patterns_resolvers/__init__.py"]
        TimePatterns["Time patterns<br/>years, decades, centuries"]
        CountryTime["Country + Year patterns<br/>nat_males_pattern"]
    end

    subgraph NewStage["New Resolvers (Priority 2)"]
        AllNew["all_new_resolvers()<br/>new_resolvers/__init__.py"]
        Jobs["Jobs Resolvers (Priority 3)<br/>jobs_resolvers/__init__.py<br/>main_jobs_resolvers()"]
        Sports["Sports Resolvers (Priority 5)<br/>sports_resolvers/__init__.py<br/>main_sports_resolvers()"]
        Nats["Nationality Resolvers (Priority 6)<br/>nationalities_resolvers/__init__.py"]
        Countries["Countries Resolvers (Priority 7)<br/>countries_names_resolvers/__init__.py"]
    end

    subgraph LegacyStage["Legacy Resolvers (Priority 4)"]
        Univ["university_resolver<br/>sub_new_resolvers/"]
        Legacy["legacy_resolvers()<br/>legacy_bots/__init__.py<br/>LegacyBotsResolver"]
    end

    subgraph OutputStage["Output Processing"]
        Fix["fixlabel()<br/>Apply Arabic grammar rules"]
        Clean["cleanse_category_label()<br/>Final formatting"]
        Result["Output: Arabic Category<br/>'تصنيف:لاعبو كرة قدم بريطانيون عام 2010'"]
    end

    Input --> ChangeCat
    ChangeCat --> Filter
    Filter -->|"valid"| AllPatterns
    Filter -->|"invalid"| Empty["Return empty string"]

    AllPatterns --> TimePatterns
    AllPatterns --> CountryTime

    AllPatterns -->|"no match"| AllNew
    AllNew --> Jobs
    Jobs -->|"no match"| Sports
    Sports -->|"no match"| Nats
    Nats -->|"no match"| Countries

    Countries -->|"no match"| Univ
    Univ -->|"no match"| Legacy

    AllPatterns -->|"match found"| Fix
    AllNew -->|"match found"| Fix
    Univ -->|"match found"| Fix
    Legacy -->|"match found"| Fix
    Legacy -->|"no match"| Empty

    Fix --> Clean
    Clean --> Result
```

**Resolution Priority Rationale**

The resolver ordering is critical to prevent conflicts:
1. **Pattern resolvers first** - Fast regex-based matching for common patterns (years, time + country)
2. **Jobs before Sports** - "football manager" must resolve as a job, not a sports category
3. **Nationalities before Countries** - "Italy political leader" uses nationality form, not country name
4. **Sports after Jobs** - Prevents job titles from being misclassified as sports terms
5. **Legacy resolvers last** - Backward compatibility for patterns not yet migrated

The system uses `@functools.lru_cache(maxsize=50000)` on the main resolver to cache results and achieve high throughput.

**Sources:** [ArWikiCats/main_processers/main_resolve.py:32-94](), [CLAUDE.md:79-92]()

---

## Core Components and File Organization

The codebase is organized into specialized modules, each handling a specific aspect of translation:

```mermaid
graph TB
    subgraph "Resolution Modules"
        NewResolvers["new_resolvers/<br/>Modern resolver implementations"]
        JobsResolvers["new_resolvers/jobs_resolvers/<br/>mens.py, womens.py, religious.py"]
        SportsResolvers["new_resolvers/sports_resolvers/<br/>raw_sports, teams, nationality+sport"]
        NatsResolvers["new_resolvers/nationalities_resolvers/<br/>nationality-based categories"]
        CountriesResolvers["new_resolvers/countries_names_resolvers/<br/>country-based categories"]
    end

    subgraph "Pattern Matching"
        Patterns["patterns_resolvers/<br/>Regex-based pattern resolvers"]
        CountryTime["country_time_pattern.py<br/>'1990s in France'"]
        NatMales["nat_males_pattern.py<br/>'British male actors'"]
    end

    subgraph "Template Engine"
        Formats["translations_formats/<br/>Template formatting system"]
        FormatBase["DataModel/model_data_base.py<br/>FormatDataBase"]
        MultiFormatter["DataModel/model_multi_data.py<br/>MultiDataFormatterBase"]
        Factories["data_with_time.py, multi_data.py<br/>Factory functions"]
    end

    subgraph "Translation Data"
        TransData["translations/<br/>Python modules with data"]
        GeoData["geo/, cities/<br/>Geographic translations"]
        JobsData["jobs/<br/>96,552 job entries"]
        SportsData["sports/<br/>431 sport records"]
        NatsData["nats/<br/>843 nationality entries"]
    end

    subgraph "JSON Source Files"
        JSONs["jsons/<br/>Raw JSON data files"]
        JSONGeo["geography/, cities/"]
        JSONJobs["jobs/"]
        JSONSports["sports/"]
        JSONNats["nationalities/"]
    end

    subgraph "Legacy System"
        Legacy["legacy_bots/<br/>Backward-compatible resolvers"]
        LegacyResolver["LegacyBotsResolver<br/>Refactored pipeline"]
        CircularDep["resolvers/<br/>Circular dependency resolution"]
    end

    subgraph "Utilities"
        Fix["fix/<br/>Normalization & cleaning"]
        Utils["utils/<br/>Helper functions"]
        Config["config.py<br/>Environment settings"]
    end

    NewResolvers --> JobsResolvers
    NewResolvers --> SportsResolvers
    NewResolvers --> NatsResolvers
    NewResolvers --> CountriesResolvers

    JobsResolvers --> Formats
    SportsResolvers --> Formats
    NatsResolvers --> Formats
    CountriesResolvers --> Formats

    Formats --> FormatBase
    Formats --> MultiFormatter
    Formats --> Factories

    JobsResolvers --> TransData
    SportsResolvers --> TransData
    NatsResolvers --> TransData
    CountriesResolvers --> TransData

    TransData --> GeoData
    TransData --> JobsData
    TransData --> SportsData
    TransData --> NatsData

    TransData --> JSONs
    JSONs --> JSONGeo
    JSONs --> JSONJobs
    JSONs --> JSONSports
    JSONs --> JSONNats

    NewResolvers --> Fix
    Patterns --> Fix
```

**Module Responsibilities**

| Module Path | Responsibility | Key Files |
|------------|----------------|-----------|
| `main_processers/` | Orchestrates resolution pipeline | `main_resolve.py` |
| `new_resolvers/` | Modern domain-specific resolvers | `jobs_resolvers/`, `sports_resolvers/`, `nationalities_resolvers/` |
| `patterns_resolvers/` | Regex-based pattern matching | `country_time_pattern.py`, `nat_males_pattern.py` |
| `translations_formats/` | Template formatting engine | `DataModel/`, factory functions |
| `translations/` | Translation data as Python modules | `geo/`, `jobs/`, `sports/`, `nats/`, `tv/` |
| `jsons/` | Raw JSON translation data | `geography/`, `jobs/`, `sports/`, `nationalities/` |
| `legacy_bots/` | Backward-compatible resolvers | `LegacyBotsResolver`, `resolvers/` |
| `fix/` | Text normalization and cleaning | `fixtitle.py`, `fixlists.py` |

**Sources:** [README.md:333-430](), [CLAUDE.md:199-220]()

---

## Public API

The system exposes four main entry points for category translation:

```mermaid
graph LR
    subgraph "Public API Functions"
        API1["resolve_arabic_category_label(category: str)<br/>→ str<br/>Returns: 'تصنيف:...'"]
        API2["resolve_label_ar(category: str)<br/>→ str<br/>Returns Arabic without 'تصنيف:' prefix"]
        API3["batch_resolve_labels(categories: List[str])<br/>→ CategoryBatchResult<br/>Returns dict of translations"]
        API4["EventProcessor.process_single(category: str)<br/>→ CategoryProcessingResult<br/>Returns detailed result with metadata"]
    end

    subgraph "Return Types"
        Result1["str - Arabic category with prefix"]
        Result2["str - Arabic label only"]
        Result3["CategoryBatchResult<br/>labels: dict<br/>no_labels: list<br/>category_patterns: dict"]
        Result4["CategoryProcessingResult<br/>original, normalized, raw_label<br/>final_label, has_label"]
    end

    API1 --> Result1
    API2 --> Result2
    API3 --> Result3
    API4 --> Result4

    style API1 fill:#f9f9f9
    style API2 fill:#f9f9f9
    style API3 fill:#f9f9f9
    style API4 fill:#f9f9f9
```

**Usage Examples**

```python
from ArWikiCats import (
    resolve_arabic_category_label,
    resolve_label_ar,
    batch_resolve_labels,
    EventProcessor
)

# Single category with prefix
label = resolve_arabic_category_label("Category:2015 British footballers")
# Returns: "تصنيف:لاعبو كرة قدم بريطانيون عام 2015"

# Single category without prefix
label = resolve_label_ar("British footballers")
# Returns: "لاعبو كرة قدم بريطانيون"

# Batch processing
categories = [
    "Category:American basketball players",
    "Category:1990s establishments in France"
]
result = batch_resolve_labels(categories)
# result.labels: dict of successful translations
# result.no_labels: list of categories without translations

# Detailed processing with metadata
processor = EventProcessor()
result = processor.process_single("Category:British footballers")
# result.original: "Category:British footballers"
# result.normalized: "british footballers"
# result.final_label: "تصنيف:لاعبو كرة قدم بريطانيون"
# result.has_label: True
```

All public API functions are exported from [ArWikiCats/__init__.py:1-40]() and documented in the package's top-level module.

**Sources:** [README.md:170-230](), [ArWikiCats/main_processers/main_resolve.py:33-100]()

---

## Translation Data Architecture

Translation data flows from raw JSON files through Python aggregator modules into specialized resolver implementations:

```mermaid
graph TB
    subgraph "Raw Data Sources (jsons/)"
        JSON1["jobs/jobs.json<br/>jobs/Jobs_22.json"]
        JSON2["sports/Sports_Keys_New.json<br/>431 sports"]
        JSON3["nationalities/nationalities_data.json<br/>843 nationalities"]
        JSON4["geography/P17_2_final_ll.json<br/>cities/popopo.json"]
        JSON5["media/Films_key_For_nat.json<br/>13,146 film entries"]
    end

    subgraph "Aggregation Layer (translations/)"
        Agg1["jobs/Jobs.py<br/>_finalise_jobs_dataset()<br/>→ 96,552 entries"]
        Agg2["sports/Sport_key.py<br/>_build_tables()<br/>→ SPORT_KEY_RECORDS"]
        Agg3["nats/Nationality.py<br/>build_lookup_tables()<br/>→ 18 lookup tables"]
        Agg4["geo/labels_country.py<br/>_build_country_label_index()<br/>→ 68,981 entries"]
        Agg5["tv/films_mslslat.py<br/>_build_gender_key_maps()"]
    end

    subgraph "Exported Data Structures"
        Export1["jobs_mens_data: 96,552<br/>jobs_womens_data<br/>Jobs_new: 1,304"]
        Export2["SPORT_KEY_RECORDS: 431<br/>SPORTS_KEYS_FOR_LABEL<br/>SPORT_JOB_VARIANTS: 571"]
        Export3["All_Nat: 843<br/>Nat_men, Nat_womens<br/>countries_from_nat: 287"]
        Export4["CITY_TRANSLATIONS_LOWER<br/>COUNTRY_LABEL_OVERRIDES<br/>US_STATES"]
    end

    subgraph "Resolver Consumption"
        Resolvers["new_resolvers/<br/>Domain-specific resolvers"]
        JobsRes["jobs_resolvers/"]
        SportsRes["sports_resolvers/"]
        NatsRes["nationalities_resolvers/"]
        CountriesRes["countries_names_resolvers/"]
    end

    JSON1 --> Agg1
    JSON2 --> Agg2
    JSON3 --> Agg3
    JSON4 --> Agg4
    JSON5 --> Agg5

    Agg1 --> Export1
    Agg2 --> Export2
    Agg3 --> Export3
    Agg4 --> Export4

    Export1 --> JobsRes
    Export2 --> SportsRes
    Export3 --> NatsRes
    Export4 --> CountriesRes

    JobsRes --> Resolvers
    SportsRes --> Resolvers
    NatsRes --> Resolvers
    CountriesRes --> Resolvers
```

**Data Processing Pipeline**

The translation data undergoes three stages of transformation:

1. **Raw JSON Storage** - Source data files in `jsons/` directory maintain original mappings
2. **Python Aggregation** - Modules in `translations/` process, merge, and transform raw data
3. **Exported Structures** - Final data structures (dicts, sets, lookup tables) used by resolvers

Key aggregation operations include:
- **Jobs**: Combines multiple JSON sources, handles gender variants, generates 96,552 male job entries
- **Sports**: Builds sport key records, job variants, team-related mappings from 431 base sports
- **Nationalities**: Creates 18 specialized lookup tables for different grammatical forms (male, female, plural, definite)
- **Geography**: Indexes 68,981 entries with city translations, country overrides, US state mappings

**Sources:** [README.md:89-113](), [changelog.md:247-294]()

---

## Performance Characteristics

The system is optimized for high-throughput batch processing with the following characteristics:

| Metric | Value | Implementation |
|--------|-------|----------------|
| **Memory Usage** | <100 MB | Optimized from 2GB in legacy system |
| **Cache Size** | 50,000 entries | `@functools.lru_cache` on `resolve_label()` |
| **Test Suite Speed** | ~23 seconds | 28,500+ tests with pytest |
| **Batch Throughput** | >5,000 categories/second | Demonstrated in `examples/5k.py` |
| **Test Coverage** | 91% | Recent improvements to legacy_bots and translations_formats |

**Caching Strategy**

The system employs multiple levels of caching:

```python
# Main resolution function with LRU cache
@functools.lru_cache(maxsize=50000)
def resolve_label(category: str, fix_label: bool = True) -> CategoryResult:
    # ... resolution logic
```

Additional caching is applied to:
- Static data loading functions in resolver modules
- Pattern compilation in `patterns_resolvers/`
- Lookup table generation in `translations/` modules

**Optimization Techniques**

1. **Early filtering** - Invalid categories rejected before entering resolver chain
2. **Priority ordering** - Most common patterns checked first
3. **Lazy loading** - Translation data loaded on-demand in some modules
4. **Pre-compiled regexes** - Pattern matchers compiled at module load time

**Sources:** [README.md:498-508](), [ArWikiCats/main_processers/main_resolve.py:32](), [changelog.md:269-293]()

---

## System Design Principles

ArWikiCats follows several key design principles that inform its architecture:

**1. Resolver Chain Priority Pattern**

Resolvers are ordered to prevent semantic conflicts. For example, "football manager" must be resolved by the jobs resolver (as an occupation) before the sports resolver can interpret it as a sports management role. The priority order is explicitly documented in [new_resolvers/__init__.py:29-57]().

**2. Template-Based Formatting**

The `FormatDataBase` class and its variants provide a template engine for pattern matching and placeholder replacement. This allows resolvers to define translation patterns like `"{nat} {sport} players"` → `"لاعبو {sport} {nat}"` with automatic substitution of Arabic equivalents.

**3. Separation of Data and Logic**

Translation data resides in `jsons/` and `translations/` directories, completely separate from resolver logic. This allows non-developers to contribute translations without modifying code.

**4. Backward Compatibility**

The `legacy_bots/` module maintains compatibility with pre-existing translation patterns through the `LegacyBotsResolver` class, which was refactored from a list-based pipeline while preserving 100% of original behavior.

**5. Comprehensive Testing**

With 28,500+ tests organized into unit, integration, and end-to-end categories, the system achieves 91% code coverage and validates translation accuracy across thousands of real-world categories.

**Sources:** [changelog.md:170-200](), [CLAUDE.md:139-143](), [README.md:434-508]()18:T439c,# Getting Started

