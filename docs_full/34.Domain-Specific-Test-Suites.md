<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [examples/data/endings.json](examples/data/endings.json)
- [examples/data/novels.json](examples/data/novels.json)
- [examples/data/television series.json](examples/data/television series.json)
- [tests/utils/dump_runner.py](tests/utils/dump_runner.py)

</details>



This page documents the domain-specific test suites that validate translation accuracy for each resolver in the ArWikiCats system. Tests are organized by translation domain (nationalities, ministers, films, countries, etc.) and use data-driven testing with pytest parametrization. For information about test data organization and JSON file structure, see [Test Data Organization](#8.1). For test execution utilities and markers, see [Test Utilities and Markers](#8.3).

## Test Suite Organization

The testing infrastructure mirrors the resolver chain architecture, with dedicated test files for each major domain. Test suites range from ~57 test cases (ministers) to 800+ cases (nationalities), reflecting the complexity and coverage needs of each resolver.

```mermaid
graph TB
    subgraph "Test Suites by Domain"
        NATS["tests/.../test_nats_v2.py<br/>Nationality Tests<br/>800+ cases"]
        YEARS["tests/.../test_labs_years.py<br/>Year Pattern Tests<br/>Decade/Century validation"]
        MINISTERS["tests/event_lists/test_ministers.py<br/>Ministers Tests<br/>57 patterns"]
        FILMS["tests/.../test_tyty.py<br/>Films Tests<br/>Genre combinations"]
        COUNTRIES["tests/.../test_countries_names2.py<br/>Country Tests<br/>24,479 entries"]
        RELATIONS["tests/.../test_relations.py<br/>Relations Tests<br/>Bilateral patterns"]
        ARLAB["tests/.../test_ar_lab_big_data.py<br/>ar_lab Tests<br/>1000+ integration"]
        PAPUA["tests/event_lists/papua_new_guinea/<br/>test_papua_new_guinea.py<br/>Country-specific"]
    end

    subgraph "Test Data Sources"
        JSON_5K["examples/data/5k.json<br/>5000+ pairs<br/>Main integration"]
        JSON_2025["examples/data/2025-11-28.json<br/>Supplementary data"]
        JSON_1K["examples/data/1k.json<br/>Elections focus"]
        JSON_NOVELS["examples/data/novels.json<br/>Novel categories"]
        JSON_FILMS["examples/data/films_with_time.json<br/>Temporal film data"]
        JSON_RELATIONS["examples/data/relations_data.json<br/>Bilateral relations"]
        JSON_TEAMS["examples/data/teams_to_test.json<br/>Sports teams"]
    end

    subgraph "Test Interface"
        RESOLVE["resolve_label_ar<br/>Main test function"]
        DUMP["one_dump_test<br/>Bulk validation"]
        DIFF["dump_diff<br/>Regression detection"]
    end

    JSON_5K --> NATS
    JSON_5K --> YEARS
    JSON_5K --> COUNTRIES
    JSON_5K --> ARLAB

    JSON_2025 --> ARLAB
    JSON_1K --> ARLAB
    JSON_NOVELS --> FILMS
    JSON_FILMS --> FILMS
    JSON_RELATIONS --> RELATIONS
    JSON_TEAMS --> ARLAB

    NATS --> RESOLVE
    YEARS --> RESOLVE
    MINISTERS --> RESOLVE
    FILMS --> RESOLVE
    COUNTRIES --> RESOLVE
    RELATIONS --> RESOLVE
    ARLAB --> RESOLVE
    PAPUA --> RESOLVE

    RESOLVE --> DUMP
    DUMP --> DIFF
```

**Sources:** [tests/event_lists/test_2.py](), [tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py](), [tests/event_lists/test_ministers.py](), [examples/data/5k.json](), [examples/data/2025-11-28.json](), [examples/data/1k.json]()

## Nationality Tests

The nationality resolver tests validate 799 nationality variants across multiple pattern types, making this the most comprehensive domain-specific test suite.

### Test Files and Coverage

| Test File | Location | Coverage | Cases |
|-----------|----------|----------|-------|
| `test_nats_v2.py` | `tests/new_resolvers/nationalities_resolvers/nationalities_v2/` | Core nationality patterns | 800+ |
| `test_nats_v2_jobs.py` | Same directory | Nationality + occupation | 200+ |
| `test_2.py` | `tests/event_lists/` | Yemeni-specific patterns | 600+ |

### Test Data Dictionary Structure

The nationality tests use multiple data dictionaries organized by grammatical form:

```mermaid
graph LR
    subgraph "Nationality Test Data Structure"
        TEST_MALES["test_data_males<br/>{'{en} non profit publishers':<br/>'ناشرون غير ربحيون {males}'}"]
        TEST_AR["test_data_ar<br/>{'{en} music groups':<br/>'فرق موسيقى {female}'}"]
        TEST_FEMALE["test_data_female_music<br/>{'{en} musical duos':<br/>'فرق موسيقية ثنائية {female}'}"]
        TEST_MALE["test_data_male<br/>{'{en} cuisine':<br/>'مطبخ {male}'}"]
        TEST_THE_MALE["test_data_the_male<br/>{'{en} occupation':<br/>'الاحتلال {the_male}'}"]
    end

    subgraph "Test Execution"
        PARAMETRIZE["@pytest.mark.parametrize<br/>'category, expected'"]
        RESOLVE_BY_NATS["resolve_by_nats<br/>function"]
        ASSERT["assert result == expected"]
    end

    TEST_MALES --> PARAMETRIZE
    TEST_AR --> PARAMETRIZE
    TEST_FEMALE --> PARAMETRIZE
    TEST_MALE --> PARAMETRIZE
    TEST_THE_MALE --> PARAMETRIZE

    PARAMETRIZE --> RESOLVE_BY_NATS
    RESOLVE_BY_NATS --> ASSERT
```

**Key Test Patterns:**

```python
# From test_nats_v2.py:8-24
test_data_males = {
    "yemeni non profit publishers": "ناشرون غير ربحيون يمنيون",
    "yemeni government officials": "مسؤولون حكوميون يمنيون",
}

test_data_ar = {
    "yemeni music groups": "فرق موسيقى يمنية",
    "yemeni rock musical groups": "فرق موسيقى روك يمنية",
}

test_data_the_male = {
    "yemeni occupation": "الاحتلال اليمني",
    "yemeni premier league": "الدوري اليمني الممتاز",
}
```

### Yemeni-Specific Test Suite

The file `test_2.py` contains an exhaustive test suite for Yemeni nationality patterns, covering 600+ category types:

```python
# From test_2.py:7-211
fast_data = {
    "yemeni sports": "ألعاب رياضية يمنية",
    "yemeni buildings": "مباني يمنية",
    "yemeni elections": "انتخابات يمنية",
    "yemeni musical groups": "فرق موسيقية يمنية",
    # ... 600+ more patterns
}
```

This data validates nationality-specific patterns across:
- Cultural categories (music, media, organizations)
- Geographic categories (islands, mountains, lakes)
- Event categories (competitions, festivals, elections)
- Occupation categories (businesspeople, journalists)

**Sources:** [tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py:1-300](), [tests/event_lists/test_2.py:1-601](), [tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2_jobs.py]()

## Ministers and Politics Tests

The ministers test suite validates translation of political office titles, focusing on grammatical correctness with the Arabic definite article (ال).

### Test Structure

```python
# From test_ministers.py:9-16
examples_1 = {
    "Ministers for foreign affairs of Papua New Guinea": "وزراء شؤون خارجية بابوا غينيا الجديدة",
    "Justice ministers of Papua New Guinea": "وزراء عدل بابوا غينيا الجديدة",
    "Agriculture ministers of Antigua and Barbuda": "وزراء زراعة أنتيغوا وباربودا",
    "Energy ministers of Antigua and Barbuda": "وزراء طاقة أنتيغوا وباربودا",
}
```

### Ministers Key Structure

The test data references `ministers_keys` dictionary which contains both definite and indefinite forms:

```mermaid
graph TB
    subgraph "ministers_keys Structure"
        KEY["ministers_keys<br/>Dictionary"]

        KEY --> AGRICULTURE["'agriculture':<br/>{no_al: 'زراعة',<br/>with_al: 'الزراعة'}"]
        KEY --> DEFENSE["'defence':<br/>{no_al: 'دفاع',<br/>with_al: 'الدفاع'}"]
        KEY --> FOREIGN["'foreign affairs':<br/>{no_al: 'شؤون خارجية',<br/>with_al: 'الشؤون الخارجية'}"]
        KEY --> JUSTICE["'justice':<br/>{no_al: 'عدل',<br/>with_al: 'العدل'}"]
    end

    subgraph "Test Validation"
        INPUT["'Justice ministers of<br/>Papua New Guinea'"]
        RESOLVE["ministers resolver"]
        OUTPUT["'وزراء عدل<br/>بابوا غينيا الجديدة'"]
    end

    INPUT --> RESOLVE
    JUSTICE --> RESOLVE
    RESOLVE --> OUTPUT
```

The test suite validates ~94 ministry types with proper article usage:

| Ministry Type | No Article (no_al) | With Article (with_al) |
|---------------|-------------------|----------------------|
| agriculture | زراعة | الزراعة |
| defence | دفاع | الدفاع |
| foreign affairs | شؤون خارجية | الشؤون الخارجية |
| justice | عدل | العدل |
| interior | داخلية | الداخلية |

**Sources:** [tests/event_lists/test_ministers.py:1-50](), [ArWikiCats/translations/politics/ministers.py:1-104]()

## Year Pattern Tests

Year pattern tests validate temporal expression translation including decades, centuries, and specific years. These tests ensure correct Arabic numeral and time period formatting.

### Test Coverage

```mermaid
graph TB
    subgraph "Year Pattern Test Types"
        DECADES["Decade Tests<br/>'1970s albums'<br/>→ 'ألبومات عقد 1970'"]
        CENTURIES["Century Tests<br/>'11th-century composers'<br/>→ 'ملحنون في القرن 11'"]
        YEARS["Specific Years<br/>'2010 births'<br/>→ 'مواليد 2010'"]
        RANGES["Year Ranges<br/>'2010-20 in British football'<br/>→ 'كرة القدم البريطانية في 2010-20'"]
    end

    subgraph "Test Data Sources"
        JSON_5K_YEARS["5k.json<br/>Temporal patterns"]
        JSON_2025_YEARS["2025-11-28.json<br/>Century patterns"]
    end

    subgraph "Validation"
        LABS_YEARS["LabsYears class"]
        CONVERT_TIME["convert_time_to_arabic<br/>function"]
        YEAR_DATA["YEAR_DATA<br/>dictionary"]
    end

    JSON_5K_YEARS --> DECADES
    JSON_5K_YEARS --> YEARS
    JSON_2025_YEARS --> CENTURIES

    DECADES --> LABS_YEARS
    CENTURIES --> LABS_YEARS
    YEARS --> LABS_YEARS
    RANGES --> LABS_YEARS

    LABS_YEARS --> CONVERT_TIME
    LABS_YEARS --> YEAR_DATA
```

### Example Test Cases

From the 5k.json dataset:

```python
# Decade patterns
"Category:2010s Massachusetts elections": "تصنيف:انتخابات ماساتشوستس في عقد 2010"
"Category:1970s in Australian tennis": "تصنيف:كرة المضرب الأسترالية في عقد 1970"

# Century patterns
"Category:11th-century composers": "تصنيف:ملحنون في القرن 11"
"Category:13th-century Italian judges": "تصنيف:قضاة إيطاليون في القرن 13"

# Year ranges
"Category:2010-20 in British football": "تصنيف:كرة القدم البريطانية في 2010-20"
```

**Sources:** [examples/data/5k.json:1-50](), [examples/data/2025-11-28.json:1-40]()

## Films and Television Tests

Films and television tests validate genre combinations, temporal patterns, and nationality-based film categories.

### Test Data Files

| File | Purpose | Example Patterns |
|------|---------|-----------------|
| `films_with_time.json` | Films with temporal data | `2010s fantasy films`, `2010 American films` |
| `television series.json` | TV series patterns | `Nigerian television series`, `2010s Swedish television series` |
| `endings.json` | TV series endings | `2010 Brazilian television series endings` |
| `novels.json` | Novel categories (film adaptations) | `2010 French novels`, `Films based on novels by Thomas Pynchon` |

### Film Test Pattern Structure

```python
# From films_with_time.json:1-7
{
    "Category:2010s fantasy novels": "تصنيف:روايات فانتازيا في عقد 2010",
    "Category:2010s science fiction novels": "تصنيف:روايات خيال علمي في عقد 2010",
    "Category:2010 fantasy novels": "تصنيف:روايات فانتازيا في 2010",
    "Category:2010s mystery films": "تصنيف:أفلام غموض في عقد 2010",
    "Category:2010s pornographic films": "تصنيف:أفلام إباحية في عقد 2010"
}
```

### Films_key_CAO Coverage

The test data validates against the `Films_key_CAO` dictionary containing 13,146 film genre and type translations:

```mermaid
graph LR
    subgraph "Film Test Categories"
        GENRE["Genre Films<br/>fantasy, science fiction,<br/>mystery, action"]
        NAT_GENRE["Nationality + Genre<br/>'American action films'"]
        TIME_GENRE["Temporal + Genre<br/>'2010s fantasy films'"]
        NAT_TIME_GENRE["All Three<br/>'2010s American action films'"]
    end

    subgraph "Test Validation"
        FILMS_KEY_CAO["Films_key_CAO<br/>13,146 entries"]
        RESOLVE_FILMS["resolve_films_labels<br/>function"]
        MULTI_FORMATTER["MultiDataFormatterBaseYear<br/>Nationality + Year"]
    end

    GENRE --> RESOLVE_FILMS
    NAT_GENRE --> RESOLVE_FILMS
    TIME_GENRE --> RESOLVE_FILMS
    NAT_TIME_GENRE --> RESOLVE_FILMS

    FILMS_KEY_CAO --> RESOLVE_FILMS
    RESOLVE_FILMS --> MULTI_FORMATTER
```

**Sources:** [examples/data/films_with_time.json](), [examples/data/television series.json](), [examples/data/endings.json](), [examples/data/novels.json:1-35]()

## Country-Specific Tests

Country-specific tests provide comprehensive validation for individual countries' categories, ensuring all nationality patterns work correctly.

### Papua New Guinea Test Suite

The Papua New Guinea test suite demonstrates the structure of country-specific testing:

```python
# From test_papua_new_guinea.py:7-22
data_skip = {
    "Category:Defunct airports in Papua New Guinea": "تصنيف:مطارات سابقة في بابوا غينيا الجديدة",
    "Category:April 2023 in Papua New Guinea": "تصنيف:بابوا غينيا الجديدة في أبريل 2023",
}

data_0 = {
    "Category:Papua New Guinea men's international soccer players": "تصنيف:لاعبو كرة قدم دوليون من بابوا غينيا الجديدة",
    "Category:Papua New Guinea women's international soccer players": "تصنيف:لاعبات كرة قدم دوليات من بابوا غينيا الجديدة",
}
```

### Country Test Coverage Pattern

```mermaid
graph TB
    subgraph "Country-Specific Test Structure"
        PEOPLE["People Categories<br/>'Papua New Guinea men'"]
        GEO["Geography Categories<br/>'Rivers of Papua New Guinea'"]
        SPORTS["Sports Categories<br/>'rugby league in PNG'"]
        POLITICS["Political Categories<br/>'Ministers of PNG'"]
        CULTURE["Cultural Categories<br/>'Museums in PNG'"]
    end

    subgraph "Test Data Organization"
        DATA_SKIP["data_skip<br/>Known issues"]
        DATA_0["data_0<br/>Main test cases"]
        DATA_1["data_1<br/>Additional patterns"]
    end

    PEOPLE --> DATA_0
    GEO --> DATA_0
    SPORTS --> DATA_0
    POLITICS --> DATA_0
    CULTURE --> DATA_0

    DATA_0 --> PARAMETRIZE["@pytest.mark.parametrize"]
    DATA_1 --> PARAMETRIZE
    DATA_SKIP --> SKIP_MARKER["@pytest.mark.skip2"]
```

The test file validates country-specific patterns including:
- **Geographic entities**: Rivers, regions, atolls, calderas
- **Political structures**: Ministers, governors, parliament constituencies
- **Sports teams**: Rugby league teams, national teams
- **Organizations**: Catholic schools, military units, missions
- **Infrastructure**: Airports, cricket grounds, military airfields

**Sources:** [tests/event_lists/papua_new_guinea/test_papua_new_guinea.py:1-200]()

## Relations Tests

Relations tests validate bilateral relationship categories between countries.

### Relations Data Structure

```python
# From relations_data.json:1-5
{
    "north macedonia–qatar relations": "تصنيف:العلاقات القطرية المقدونية الشمالية",
    "north macedonia–serbia border crossings": "تصنيف:معابر الحدود الصربية المقدونية الشمالية",
    "north macedonia–serbia border": "تصنيف:الحدود الصربية المقدونية الشمالية",
    "north macedonia–serbia relations": "تصنيف:العلاقات الصربية المقدونية الشمالية"
}
```

### Relations Pattern Types

```mermaid
graph TB
    subgraph "Relations Test Patterns"
        REL["Basic Relations<br/>'Country A-Country B relations'"]
        BORDER["Border Patterns<br/>'Country A-Country B border'"]
        CROSSINGS["Border Crossings<br/>'Country A-Country B border crossings'"]
        TREATIES["Treaties<br/>'Country A-Country B treaties'"]
    end

    subgraph "Validation Logic"
        FORMAT_DOUBLE["FormatDataDoubleV2<br/>Dual placeholders"]
        PUT_LABEL_LAST["put_label_last<br/>Reorder labels"]
        COUNTRY_DATA["formatted_data_en_ar<br/>Country translations"]
    end

    REL --> FORMAT_DOUBLE
    BORDER --> FORMAT_DOUBLE
    CROSSINGS --> FORMAT_DOUBLE
    TREATIES --> FORMAT_DOUBLE

    FORMAT_DOUBLE --> PUT_LABEL_LAST
    FORMAT_DOUBLE --> COUNTRY_DATA
```

**Sources:** [examples/data/relations_data.json:1-10]()

## Integration Tests (ar_lab)

The `ar_lab` test suite provides end-to-end integration testing across all resolvers with 1000+ complex test cases.

### Test Data Sources for Integration

```mermaid
graph TB
    subgraph "Integration Test Data"
        JSON_5K_INT["5k.json<br/>5000+ pairs<br/>Primary integration"]
        JSON_2025_INT["2025-11-28.json<br/>Supplementary"]
        JSON_1K_INT["1k.json<br/>Elections focus"]
        JSON_TEAMS["teams_to_test.json<br/>Sports teams"]
    end

    subgraph "Integration Test Flow"
        AR_LAB_TEST["test_ar_lab_big_data.py<br/>1000+ cases"]
        RESOLVE_LABEL["resolve_label_ar<br/>Full pipeline"]
        ALL_RESOLVERS["All 7 resolvers<br/>Year, Nat, Country,<br/>Job, Sport, Film, Minister"]
    end

    subgraph "Validation"
        SEPARATOR_FIX["separator_lists_fixing<br/>Preposition insertion"]
        ADD_IN_TAB["add_in_tab<br/>Grammar corrections"]
        FIXLABEL["fixlabel<br/>Article agreement"]
    end

    JSON_5K_INT --> AR_LAB_TEST
    JSON_2025_INT --> AR_LAB_TEST
    JSON_1K_INT --> AR_LAB_TEST
    JSON_TEAMS --> AR_LAB_TEST

    AR_LAB_TEST --> RESOLVE_LABEL
    RESOLVE_LABEL --> ALL_RESOLVERS
    ALL_RESOLVERS --> SEPARATOR_FIX
    SEPARATOR_FIX --> ADD_IN_TAB
    ADD_IN_TAB --> FIXLABEL
```

### Integration Test Coverage

The integration tests validate:

1. **Multi-domain categories**: Categories requiring multiple resolvers (e.g., "2010s American football coaches")
2. **Complex grammar**: Categories with prepositions, separators, and Arabic article agreement
3. **Edge cases**: Unusual category structures, multiple separators, special characters
4. **End-to-end accuracy**: Complete translation pipeline from raw input to final Arabic output

**Example Complex Cases:**

```python
# From 5k.json - Categories requiring multiple resolvers
"Category:2010s American sports-people": "تصنيف:رياضيون أمريكيون في عقد 2010"  # Year + Nationality
"Category:Ministers for foreign affairs of Papua New Guinea": "تصنيف:وزراء شؤون خارجية بابوا غينيا الجديدة"  # Ministers + Country
"Category:2010s fantasy novels": "تصنيف:روايات فانتازيا في عقد 2010"  # Year + Film genre
```

**Sources:** [examples/data/5k.json:1-100](), [examples/data/2025-11-28.json:1-50](), [examples/data/1k.json:1-50](), [examples/data/teams_to_test.json]()

## Test Execution Patterns

All domain-specific tests follow a consistent execution pattern using pytest parametrization and helper utilities.

### Common Test Structure

```python
# Common pattern across all test files
import pytest
from load_one_data import dump_diff, one_dump_test
from ArWikiCats import resolve_label_ar

test_data = {
    "input category": "expected output",
    # ... more test cases
}

@pytest.mark.parametrize("category, expected", test_data.items())
def test_domain(category, expected):
    result = resolve_label_ar(category)
    assert result == expected
```

### Test Helper Functions

```mermaid
graph LR
    subgraph "Test Helper Functions"
        ONE_DUMP["one_dump_test<br/>Bulk validation<br/>Run all test cases"]
        DUMP_DIFF["dump_diff<br/>Regression detection<br/>Compare results"]
        RESOLVE["resolve_label_ar<br/>Main test interface<br/>Full pipeline"]
    end

    subgraph "Test Execution"
        TEST_FILE["test_*.py"]
        PARAMETRIZE["@pytest.mark.parametrize"]
        ASSERT["assert result == expected"]
    end

    subgraph "Test Output"
        SUCCESS["Test Pass"]
        REGRESSION["Regression Report<br/>Expected vs Actual"]
        COVERAGE["Coverage Metrics"]
    end

    TEST_FILE --> PARAMETRIZE
    PARAMETRIZE --> RESOLVE
    RESOLVE --> ASSERT
    ASSERT --> SUCCESS
    ASSERT --> REGRESSION

    TEST_FILE --> ONE_DUMP
    ONE_DUMP --> DUMP_DIFF
    DUMP_DIFF --> REGRESSION
    DUMP_DIFF --> COVERAGE
```

### Test Markers Usage

Tests use pytest markers for selective execution:

| Marker | Purpose | Usage |
|--------|---------|-------|
| `@pytest.mark.fast` | Quick unit tests | Development testing |
| `@pytest.mark.slow` | Comprehensive tests | CI/CD validation |
| `@pytest.mark.dump` | Full dataset validation | Regression testing |
| `@pytest.mark.skip2` | Known issues | Temporary exclusion |
| `@pytest.mark.parametrize` | Data-driven tests | All domain tests |

**Sources:** [tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py:1-10](), [tests/event_lists/test_2.py:1-10](), [tests/event_lists/test_ministers.py:1-10]()

## Domain Coverage Summary

The following table summarizes test coverage across all domains:

| Domain | Test File(s) | Test Cases | Data Sources | Importance |
|--------|-------------|------------|--------------|------------|
| Nationalities | `test_nats_v2.py`, `test_2.py`, `test_nats_v2_jobs.py` | 800+ | 799 nationality variants | 129.38 |
| Ministers/Politics | `test_ministers.py` | 57 | `ministers_keys` (94 entries) | 88.16 |
| Films/TV | `test_tyty.py`, films JSON files | 500+ | `Films_key_CAO` (13,146 entries) | 85.39 |
| Year Patterns | `test_labs_years.py` | 300+ | `YEAR_DATA` dictionary | 81.33 |
| Countries | `test_countries_names2.py`, country-specific tests | 1000+ | `NEW_P17_FINAL` (24,479 entries) | 72.63 |
| Relations | `test_relations.py` | 200+ | `relations_data.json` | 65.89 |
| Integration | `test_ar_lab_big_data.py` | 5000+ | Multiple JSON files | 86.76 |

This comprehensive test coverage ensures translation accuracy across ~100,000+ category patterns, with domain-specific validation guaranteeing correct grammatical forms, preposition usage, and Arabic article agreement.

**Sources:** [tests/event_lists/test_2.py](), [tests/new_resolvers/nationalities_resolvers/nationalities_v2/test_nats_v2.py](), [tests/event_lists/test_ministers.py](), [examples/data/5k.json](), [ArWikiCats/translations/politics/ministers.py]()3a:T5f71,# Test Utilities

